{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools  \n",
    "import numpy                 as np\n",
    "import tensorflow            as tf\n",
    "import tensorflow_addons     as tfa\n",
    "import matplotlib.pyplot     as plt\n",
    "import pandas                as pd\n",
    "import gudhi                 as gd\n",
    "import gudhi.representations as sktda\n",
    "import sys\n",
    "\n",
    "from difftda                              import *\n",
    "from gudhi.representations.vector_methods import Atol as atol\n",
    "from gudhi.representations.kernel_methods import SlicedWassersteinKernel as swk\n",
    "from gudhi.wasserstein                    import wasserstein_distance\n",
    "from gudhi.representations                import pairwise_persistence_diagram_distances as ppdd\n",
    "from mpl_toolkits.mplot3d                 import Axes3D\n",
    "from scipy.linalg                         import expm\n",
    "from scipy.io                             import loadmat\n",
    "from scipy.sparse                         import csgraph\n",
    "from scipy.linalg                         import eigh\n",
    "from sklearn.base                         import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics                      import pairwise_distances, accuracy_score\n",
    "from sklearn.manifold                     import MDS, LocallyLinearEmbedding, SpectralEmbedding\n",
    "from sklearn.preprocessing                import MinMaxScaler, Normalizer, LabelEncoder\n",
    "from sklearn.pipeline                     import Pipeline, FeatureUnion\n",
    "from sklearn.svm                          import SVC\n",
    "from sklearn.ensemble                     import RandomForestClassifier\n",
    "from sklearn.neighbors                    import KNeighborsClassifier\n",
    "from sklearn.model_selection              import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.cluster                      import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "angles = np.random.uniform(0,2*np.pi,100)\n",
    "X = np.hstack([ np.cos(angles)[:,None], np.sin(angles)[:,None] ])\n",
    "dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array([[0.1,0.],[1.5,1.6]])\n",
    "X = np.array([[0.1,0.],[1.5,1.5],[0.,1.6]])\n",
    "dim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexRipsModel(tf.keras.Model):\n",
    "    def __init__(self, X, mel=10, dim=dim, card=50):\n",
    "        super(IndexRipsModel, self).__init__()\n",
    "        self.X = X\n",
    "        self.mel = mel\n",
    "        self.dim = dim\n",
    "        self.card = card\n",
    "        \n",
    "    def call(self, D):\n",
    "        m, d, c = self.mel, self.dim, self.card\n",
    "        \n",
    "        # Compute distance matrix\n",
    "        DX = tfa.losses.metric_learning.pairwise_distance(self.X)\n",
    "        DXX = tf.reshape(DX, [1, DX.shape[0], DX.shape[1]])\n",
    "        \n",
    "        # Turn numpy function into tensorflow function\n",
    "        RipsTF = lambda DX: tf.numpy_function(Rips, [DX, m, d, c], [tf.int32 for _ in range(4*c)])\n",
    "        \n",
    "        # Compute vertices associated to positive and negative simplices \n",
    "        # Don't compute gradient for this operation\n",
    "        ids = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(RipsTF,DXX,dtype=[tf.int32 for _ in range(4*c)]))\n",
    "        \n",
    "        # Get persistence diagram by simply picking the corresponding entries in the distance matrix\n",
    "        dgm = tf.reshape(tf.gather_nd(D, tf.reshape(ids, [2*c,2])), [c,2])\n",
    "        return dgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTF = tf.Variable(X, tf.float32)\n",
    "lr = 1\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "num_epochs = 1\n",
    "losses, Dgs, Xs, grads = [], [], [], []\n",
    "for epoch in range(num_epochs+1):\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        Dg = RipsModel(X=XTF, mel=10, dim=dim, card=10).call()\n",
    "        loss = -wasserstein_distance(Dg, tf.constant(np.empty([0,2])), order=1, enable_autodiff=True)\n",
    "        \n",
    "    Dgs.append(Dg.numpy())            \n",
    "    Xs.append(XTF.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "    \n",
    "    gradients = tape.gradient(loss, [XTF])\n",
    "    grads.append(gradients[0].numpy())\n",
    "    optimizer.apply_gradients(zip(gradients, [XTF]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_to_move = np.argwhere(np.linalg.norm(grads[0], axis=1) != 0).ravel()\n",
    "plt.figure()\n",
    "for pt in pts_to_move:\n",
    "    plt.arrow(Xs[0][pt,0], Xs[0][pt,1], -lr*grads[0][pt,0], -lr*grads[0][pt,1], color='blue',\n",
    "              length_includes_head=True, head_length=.05, head_width=.1, zorder=10)\n",
    "plt.scatter(Xs[0][:,0], Xs[0][:,1], c='red', s=50, alpha=.2,  zorder=3)\n",
    "plt.scatter(Xs[0][pts_to_move,0], Xs[0][pts_to_move,1], c='red',   s=150, marker='o', zorder=2, alpha=.7, label='Step i')\n",
    "plt.scatter(Xs[1][pts_to_move,0], Xs[1][pts_to_move,1], c='green', s=150, marker='o', zorder=1, alpha=.7, label='Step i+1')\n",
    "plt.axis('square')\n",
    "#plt.xlim([-.7,2.3])\n",
    "#plt.ylim([-.7,2.3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the dimension reduction experiment, where we improve dimension reduction with autoencoders using 1-dimensional homology. Use `use_reg=True` if you want to add a topological loss to the autoencoder, and `common_topo_autoencoder=True` if you want to use the topological loss of Moor et al (https://arxiv.org/abs/1906.00722)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = True\n",
    "common_topo_autoencoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "eps = .1\n",
    "Rell1x, Rell1y = 1, 1\n",
    "theta1x, theta1y = 0, 0\n",
    "Rell2x, Rell2y = .8, .6\n",
    "theta2x, theta2y = -np.pi/4, -np.pi/6\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "angles = np.linspace(0,2*np.pi,N)[:,np.newaxis]\n",
    "Rcic = 1.\n",
    "cic = np.hstack([np.zeros([N,1]), \n",
    "                 Rcic * np.cos(angles) + eps * np.random.uniform(-1,1,(N,1)),\n",
    "                 Rcic * np.sin(angles) + eps * np.random.uniform(-1,1,(N,1))])\n",
    "ell1 = np.hstack([\n",
    "    Rell1x * np.cos(angles) + eps * np.random.uniform(-1,1,(N,1)),\n",
    "    Rell1y * np.sin(angles) + eps * np.random.uniform(-1,1,(N,1)),\n",
    "    np.zeros([N,1])])\n",
    "R1x = np.array([[1,0,0],[0,np.cos(theta1x),-np.sin(theta1x)],[0,np.sin(theta1x),np.cos(theta1x)]])\n",
    "R1y = np.array([[np.cos(theta1y),0,np.sin(theta1y)],[0,1,0],[-np.sin(theta1y),0,np.cos(theta1y)]])\n",
    "ell2 = np.hstack([\n",
    "    Rell2x * np.cos(angles) + eps * np.random.uniform(-1,1,(N,1)),\n",
    "    Rell2y * np.sin(angles) + eps * np.random.uniform(-1,1,(N,1)),\n",
    "    np.zeros([N,1])])\n",
    "R2x = np.array([[1,0,0],[0,np.cos(theta2x),-np.sin(theta2x)],[0,np.sin(theta2x),np.cos(theta2x)]])\n",
    "R2y = np.array([[np.cos(theta2y),0,np.sin(theta2y)],[0,1,0],[-np.sin(theta2y),0,np.cos(theta2y)]])\n",
    "ell1 = ell1 + np.array([[.3,.3,0.]])\n",
    "ell2 = ell2 + np.array([[-.1,-.1,0.]])\n",
    "Xinit = np.vstack([cic, cic[0,:] + np.dot(np.dot(ell1,R1x),R1y)]) #, cic[int(N/2),:] + np.dot(np.dot(ell2,R2x),R2y)])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(Xinit[:,0], Xinit[:,1], Xinit[:,2], \n",
    "                c=np.concatenate([np.ones([N]),2*np.ones([N])]), #3*np.ones([N])]), \n",
    "                s=5, cmap='rainbow')\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "ax.grid(False)\n",
    "plt.title('Initial point cloud')\n",
    "plt.savefig('dimredinit_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rot = []\n",
    "for pt in Xinit:\n",
    "    M = np.zeros((3,3))\n",
    "    M[0,1] = pt[0]\n",
    "    M[0,2] = pt[1]\n",
    "    M[1,0] = -pt[0]\n",
    "    M[2,0] = -pt[1]\n",
    "    M[1,2] = pt[2]\n",
    "    M[2,1] = -pt[2]\n",
    "    Rot.append(expm(M).flatten())\n",
    "Xinit = np.asarray(Rot)\n",
    "Dinit = pairwise_distances(Xinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexRipsModel(tf.keras.Model):\n",
    "    def __init__(self, X, mel=12, dim=1, card=50):\n",
    "        super(IndexRipsModel, self).__init__()\n",
    "        self.X = X\n",
    "        self.mel = mel\n",
    "        self.dim = dim\n",
    "        self.card = card\n",
    "        \n",
    "    def call(self, D):\n",
    "        m, d, c = self.mel, self.dim, self.card\n",
    "        \n",
    "        # Compute distance matrix\n",
    "        DX = tfa.losses.metric_learning.pairwise_distance(self.X)\n",
    "        DXX = tf.reshape(DX, [1, DX.shape[0], DX.shape[1]])\n",
    "        \n",
    "        # Turn numpy function into tensorflow function\n",
    "        RipsTF = lambda DX: tf.numpy_function(Rips, [DX, m, d, c], [tf.int32 for _ in range(4*c)])\n",
    "        \n",
    "        # Compute vertices associated to positive and negative simplices \n",
    "        # Don't compute gradient for this operation\n",
    "        ids = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(RipsTF,DXX,dtype=[tf.int32 for _ in range(4*c)]))\n",
    "        \n",
    "        # Get persistence diagram by simply picking the corresponding entries in the distance matrix\n",
    "        dgm = tf.reshape(tf.gather_nd(D, tf.reshape(ids, [2*c,2])), [c,2])\n",
    "        return dgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = gd.RipsComplex(distance_matrix=Dinit, max_edge_length=2).create_simplex_tree(max_dimension=2)\n",
    "st.persistence()\n",
    "Dg1init = st.persistence_intervals_in_dimension(1)\n",
    "\n",
    "initial_learning_rate = 0.05\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=1e5, decay_rate=0.99)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "Dg1iTF = tf.constant(np.array(Dg1init,dtype=np.float32))\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "autoencoder = tf.keras.models.Sequential()\n",
    "autoencoder.add(tf.keras.Input(shape=(Xinit.shape[1],)))\n",
    "autoencoder.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "autoencoder.add(tf.keras.layers.BatchNormalization())\n",
    "autoencoder.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "autoencoder.add(tf.keras.layers.BatchNormalization())\n",
    "autoencoder.add(tf.keras.layers.Dense(2, activation=None))\n",
    "autoencoder.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "autoencoder.add(tf.keras.layers.BatchNormalization())\n",
    "autoencoder.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "autoencoder.add(tf.keras.layers.BatchNormalization())\n",
    "autoencoder.add(tf.keras.layers.Dense(Xinit.shape[1], activation=None))\n",
    "\n",
    "reduced_data = tf.keras.Model(autoencoder.input, autoencoder.get_layer(index=4).output) \n",
    "\n",
    "Xred = reduced_data.predict(Xinit)\n",
    "st = gd.RipsComplex(Xred, max_edge_length=2).create_simplex_tree(max_dimension=2)\n",
    "st.persistence()\n",
    "Dg1red = st.persistence_intervals_in_dimension(1)\n",
    "\n",
    "a3 = 1\n",
    "a2 = 1e1 if use_reg else 0\n",
    "num_epochs = 200\n",
    "losses, l2s, l3s, Dgs1, Xs = [], [], [], [], []\n",
    "for epoch in range(num_epochs+1):\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        XXinit = autoencoder(Xinit)\n",
    "        Xred = reduced_data(Xinit)\n",
    "\n",
    "        if use_reg and epoch >= 0:\n",
    "            if common_topo_autoencoder:\n",
    "                XinitTF = tf.convert_to_tensor(Xinit, dtype=tf.float32)\n",
    "                DZ = tfa.losses.metric_learning.pairwise_distance(Xred)\n",
    "                DX = tfa.losses.metric_learning.pairwise_distance(XinitTF)\n",
    "                DZX = IndexRipsModel(X=Xred, mel=3.,  dim=0, card=50).call(DX)\n",
    "                DZZ = IndexRipsModel(X=Xred, mel=3.,  dim=0, card=50).call(DZ)\n",
    "                DXX = IndexRipsModel(X=XinitTF, mel=3., dim=0, card=50).call(DX)\n",
    "                DXZ = IndexRipsModel(X=XinitTF, mel=3., dim=0, card=50).call(DZ)\n",
    "            else:\n",
    "                Dg1 = RipsModel(X=Xred, mel=3., dim=1, card=50).call()\n",
    "                Dgs1.append(Dg1.numpy())\n",
    "            \n",
    "        Xs.append(Xred)\n",
    "        l3 = a3*tf.math.reduce_sum(tf.square(Xinit-XXinit))\n",
    "\n",
    "        if use_reg and epoch >= 0:\n",
    "            if common_topo_autoencoder:\n",
    "                l2 = tf.math.reduce_sum(tf.square(DXX-DXZ)) + tf.math.reduce_sum(tf.square(DZX-DZZ))\n",
    "            else:\n",
    "                l2 = a2*wasserstein_distance(Dg1, Dg1iTF, order=1, enable_autodiff=True)\n",
    "                \n",
    "        loss = l3\n",
    "\n",
    "        if use_reg and epoch >= 0:\n",
    "            loss = loss + l2\n",
    "    \n",
    "    gradients = tape.gradient(loss, autoencoder.trainable_variables)\n",
    "    tf.random.set_seed(epoch)\n",
    "    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    if use_reg and epoch >= 0:\n",
    "        l2s.append(l2.numpy())\n",
    "    l3s.append(l3.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title('Losses')\n",
    "plt.savefig('dimredloss_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(Xs[-1][:,0], Xs[-1][:,1], s=10, c=np.concatenate([np.ones([N]),2*np.ones([N])]), cmap='rainbow')\n",
    "plt.title('LowD point cloud at epoch ' + str(epoch))\n",
    "plt.savefig('dimredafter_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_reg and not common_topo_autoencoder:\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(Dg1init[:,0],  Dg1init[:,1],  label='initial')\n",
    "    plt.scatter(Dg1red[:,0],   Dg1red[:,1],   label='lowD')\n",
    "    plt.scatter(Dgs1[-1][:,0], Dgs1[-1][:,1], label='trained lowD')\n",
    "    plt.plot([0,1.5],[0,1.5])\n",
    "    plt.axis('square')\n",
    "    plt.title('Dim. 1 PD')\n",
    "    plt.legend()\n",
    "    plt.savefig('dimreddg10_' + str(use_reg) + '.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(Dg1init[:,0],  Dg1init[:,1],  label='initial')\n",
    "    plt.scatter(Dg1red[:,0],   Dg1red[:,1],   label='lowD')\n",
    "    for D in Dgs1[0:-1]:\n",
    "        plt.scatter(D[:,0], D[:,1], s=20, marker='D', alpha=0.1)\n",
    "    plt.scatter(Dgs1[-1][:,0], Dgs1[-1][:,1], s=40, marker='D', c='green', label='trained lowD')\n",
    "    plt.plot([0,1.5],[0,1.5])\n",
    "    plt.axis('square')\n",
    "    plt.title('Dim. 1 PD')\n",
    "    plt.legend()\n",
    "    plt.savefig('dimreddg11_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the linear regression experiment, where we recover hidden coefficients using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n, p = 50, 100\n",
    "betastar = np.concatenate([np.linspace(-1.,1.,33) for _ in range(3)] + [[-1.]])\n",
    "X = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "\n",
    "Y = np.matmul(X, betastar) + .05 * np.random.randn(n)\n",
    "X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(p-1):\n",
    "    stbase.insert([i,i+1], -1e10)\n",
    "f = open('data/beta_simplextree.txt', 'w')\n",
    "for (s,_) in stbase.get_filtration():\n",
    "    for v in s:\n",
    "        f.write(str(v) + \" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "betainit = np.random.uniform(low=-1., high=1., size=[p])\n",
    "betainit[np.array([25,60,99])] = np.array([-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar)\n",
    "plt.title('Ground-truth coefficients')\n",
    "plt.savefig('reggt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betainit)\n",
    "plt.title('Coefficients at epoch 0')\n",
    "plt.savefig('reginit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase='data/beta_simplextree.txt', dim=0, card=100)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 1e4, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "\n",
    "beta_stdtop = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('regloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('regdg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase='data/beta_simplextree.txt', dim=0, card=100)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 0, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "                 \n",
    "beta_stdtot = -betas[-1]\n",
    "\n",
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase='data/beta_simplextree.txt', dim=0, card=100)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 2, 0, 0\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "                     \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "    \n",
    "beta_std = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar, label='ground-truth')\n",
    "plt.plot(beta_std, label='MSE')\n",
    "plt.plot(beta_stdtot, label='MSE+TV')\n",
    "plt.plot(beta_stdtop, label='MSE+TV+TOP')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('regafter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEstd, MSEtop, MSEtot = [], [], []\n",
    "for s in range(1000):\n",
    "    np.random.seed(s)\n",
    "    Xnew = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "    Ynew = np.matmul(Xnew, betastar)\n",
    "    mse_std = np.square(np.matmul(Xnew, beta_std) - Ynew).sum()\n",
    "    mse_tot = np.square(np.matmul(Xnew, beta_stdtot) - Ynew).sum()\n",
    "    mse_top = np.square(np.matmul(Xnew, beta_stdtop) - Ynew).sum()\n",
    "    MSEstd.append(mse_std)\n",
    "    MSEtot.append(mse_tot)\n",
    "    MSEtop.append(mse_top)\n",
    "    \n",
    "plt.figure()\n",
    "plt.boxplot([MSEstd, MSEtot, MSEtop], labels=['MSE', 'MSE+TV', 'MSE+TV+TOP'])\n",
    "plt.title('MSE on random test sets')\n",
    "plt.savefig('regmse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the image experiment, where we remove the noise of an image using 0-dimensional homology. Use `use_reg=True` if you want to use a topological loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.array(pd.read_csv('data/mnist_test.csv', header=None, sep=','), dtype=np.float32)\n",
    "idx = np.argwhere(I[:,0] == 8)\n",
    "image = np.reshape(-I[idx[8],1:], [28,28])\n",
    "image = (image-image.min())/(image.max()-image.min())\n",
    "image_clean = np.array(image)\n",
    "image[2:5,2:5]        -= 0.6\n",
    "image[25:27,25:27]    -= 0.6\n",
    "image[25:27,2:5]      -= 0.6\n",
    "image[1:4,24:26]      -= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image, cmap='Greys')\n",
    "plt.title('Image at epoch 0')\n",
    "plt.savefig('imbefore_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=np.array(image, dtype=np.float32), trainable=True)\n",
    "model = CubicalModel(X, dim=0, card=100)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-3, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, empty = [], [], np.empty([0,2])\n",
    "alpha = 10.\n",
    "gamma = 1. if use_reg else 0\n",
    "for epoch in range(3000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        if use_reg:\n",
    "            #loss = alpha * tf.math.reduce_sum(tf.abs(dgm[:,1]-dgm[:,0])) + \\\n",
    "            loss = gamma * tf.math.reduce_sum(tf.math.minimum(tf.abs(model.X), tf.abs(1.-model.X)))\n",
    "        else:\n",
    "            loss = alpha * tf.math.reduce_sum(tf.abs(dgm[:,1]-dgm[:,0]))\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(model.X.numpy(), cmap='Greys')\n",
    "plt.title('Image at epoch ' + str(epoch))\n",
    "plt.savefig('imafter_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('imloss_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('imdg_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the point cloud experiment, where we optimize loops in a point cloud using 1-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "Xinit = np.array(np.random.uniform(high=1., low=-1., size=(300,2)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(Xinit[:,0], Xinit[:,1])\n",
    "plt.title('Point cloud at epoch 0')\n",
    "plt.savefig('pcinit_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "model = RipsModel(X=X, mel=12., dim=1, card=50)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "for epoch in range(1000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        if use_reg:\n",
    "            loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0]))) + tf.reduce_sum(tf.maximum(tf.abs(X)-1, 0))\n",
    "        else:\n",
    "            loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0])))\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(model.X.numpy()[:,0], model.X.numpy()[:,1])\n",
    "plt.title('Point cloud at epoch ' + str(epoch))\n",
    "plt.savefig('pcafter_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('pcloss_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for dg in dgms[:5:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='green')\n",
    "plt.plot([-0.,.2], [-0.,.2])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('pcdg_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the noisy point cloud experiment, where we optimize the connected components of a noisy point cloud using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, epsilon, nout = 100, .2, 3\n",
    "x, y = np.cos(np.linspace(0,2*np.pi,n)), np.sin(np.linspace(0,2*np.pi,n))\n",
    "np.random.seed(10)\n",
    "ex, ey = np.random.uniform(low=-epsilon,high=epsilon,size=n), np.random.uniform(low=-epsilon,high=epsilon,size=n)\n",
    "outliers = np.random.uniform(low=-.7, high=.7, size=(nout,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x+ex, y+ey)\n",
    "plt.scatter(outliers[:,0], outliers[:,1])\n",
    "plt.title('Point cloud at epoch 0')\n",
    "plt.savefig('noisypcinit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = gd.RipsComplex(distance_matrix=pairwise_distances(np.hstack([x[:,np.newaxis],y[:,np.newaxis]])), max_edge_length=2.).create_simplex_tree(max_dimension=2)\n",
    "st.persistence()\n",
    "D = np.array(st.persistence_intervals_in_dimension(0), dtype=np.float32)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinit = np.array(np.vstack([np.hstack([(x+ex)[:,np.newaxis], (y+ey)[:,np.newaxis]]),outliers]), dtype=np.float32)\n",
    "\n",
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "model = RipsModel(X=X, mel=2., dim=0, card=150)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        loss = tf.square(wasserstein_distance(dgm, tf.constant(D), order=2, enable_autodiff=True))\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(model.X.numpy()[:,0], model.X.numpy()[:,1])\n",
    "plt.title('Point cloud at epoch ' + str(epoch))\n",
    "plt.savefig('noisypcafter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('noisypcloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('noisypcdg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the 3D shape experiment, where we optimize the values on a 3D shape using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, coord = np.loadtxt('data/human_faces', dtype=float)[:,1:], np.loadtxt('data/human_coords', dtype=float)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(len(faces)):\n",
    "    stbase.insert(faces[i,:], -1e10)\n",
    "f = open('data/human_simplextree.txt', 'w')\n",
    "for (s,_) in stbase.get_filtration():\n",
    "    for v in s:\n",
    "        f.write(str(v) + \" \")\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "Finit = coord[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap('rainbow')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=Finit[::step], s=2, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title('Function at epoch 0')\n",
    "plt.savefig('d3sinit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = tf.Variable(initial_value=np.array(Finit, dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(F, stbase='data/human_simplextree.txt', dim=0, card=50)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "alpha, gamma = 1., .001\n",
    "for epoch in range(3000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = model.call()\n",
    "        loss = alpha * tf.square(wasserstein_distance(dgm, tf.constant(np.array([[-.98,-.03]], dtype=np.float32)), order=2, enable_autodiff=True))               \n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    np.random.seed(epoch)\n",
    "    gradients = [tf.convert_to_tensor(gradients[0]) + np.random.normal(loc=0., scale=sigma, size=gradients[0].dense_shape)]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap('rainbow')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=F.numpy()[::step], s=1, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title('Function at epoch ' + str(epoch))\n",
    "plt.savefig('d3safter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('d3sloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1:2]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('d3sdg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
